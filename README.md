Conditional Polygon Coloring with a U-Net in PyTorch

This repository contains the complete project for the Ayna ML Assignment. The goal was to build and train a deep learning model that can take an image of a polygon and a color name as input, and produce an image of that polygon filled with the specified color.

The final model is a general-purpose shape colorizer, capable of handling arbitrary polygon shapes, not just those seen during training. This was achieved through a combination of a U-Net architecture, a channel-wise conditioning strategy, and a robust data augmentation pipeline.

<br>


<p align="center">Example of the model taking a shape and a color name to produce the final output.</p>

<br>

Features

U-Net Architecture: Implemented from scratch in PyTorch for high-fidelity image-to-image translation.

Conditional Generation: The model is conditioned on text-based color names using a channel-wise concatenation strategy.

Data Augmentation: The training pipeline includes a synchronized augmentation process (rotation, scaling, translation) to ensure the model is robust and generalizes well to unseen shapes and orientations.

Experiment Tracking: All experiments, hyperparameter tuning, and model versions were rigorously tracked using Weights & Biases. You can view the full project dashboard here:

W&B Project Link: https://wandb.ai/hibifovohig3-add/ayna-ml-polygon-coloring

Ready-for-Inference: The repository includes a pre-trained model file (best_model_augmented.pth) and a Google Colab notebook ready for immediate use.

Getting Started: Inference with Google Colab

The easiest way to use this model and see it in action is to use the provided Google Colab notebook. It includes all the necessary code to download the model, set up the environment, and run inference on any shape you provide.

Prerequisites

A Google Account to access Google Colab.

Instructions

Open the Notebook: Click the badge below to open the main project notebook directly in Google Colab.

![alt text](https://colab.research.google.com/assets/colab-badge.svg)

Connect to a Runtime: In the Colab interface, click the "Connect" button in the top-right corner. For faster inference, you can select a GPU runtime by navigating to Runtime -> Change runtime type -> Hardware accelerator -> GPU.

Run the Inference Cells: The notebook is divided into sections. For inference, you only need to run the cells under the "FINAL, STREAMLINED INFERENCE CELL" section.

The first cell in this section will set up the environment and download the pre-trained model.

The second cell contains the code to perform the inference.

Customize Your Input: In the final cell, you can easily change the input image and color:

To use a new shape:

In the Colab file browser (folder icon on the left), upload your custom shape image (e.g., my_star.png).

Change the INPUT_POLYGON_PATH variable to point to your file:

Generated python
INPUT_POLYGON_PATH = '/content/my_star.png'


To use a new color:

Choose a color from the "Available colors" list printed in the cell output.

Change the INPUT_COLOR variable:

Generated python
INPUT_COLOR = 'magenta'
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

View the Result: After running the cell, the notebook will display your input shape and the final colored output generated by the model.

Project Structure

For those interested in the full project development, the repository contains:```
.
├── checkpoints/
│ └── best_model_augmented.pth # The final, pre-trained model weights
├── dataset/
│ ├── training/ # Training data (images, JSON)
│ └── validation/ # Validation data (images, JSON)
├── Ayna_ML_Assignment.ipynb # The main Google Colab notebook with all code
└── README.md # This file

Generated code
The `Ayna_ML_Assignment.ipynb` notebook is comprehensive and includes the code for all experiments, including the inferior methods tested (MSELoss, Bottleneck Injection) and the final, augmented training run. This provides a complete, transparent view of the entire development process.

## **Key Learnings**

*   **Synchronized Augmentation is a Game-Changer:** This was the most impactful technique for improving model robustness, transforming it from a simple memorizer to a general-purpose tool.
*   **Systematic Experimentation Builds Better Models:** By testing and logging multiple approaches, we generated concrete evidence to prove *why* the final combination of techniques (L1Loss, Channel-wise Conditioning, LR Scheduling, and Augmentation) was the optimal choice.
*   **The Data Pipeline is Part of the Model:** Ensuring a consistent, bug-free data pipeline (especially regarding shared components like encoders) is as important as designing the neural network itself.
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
IGNORE_WHEN_COPYING_END
